{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1022eab1-ff30-41a3-8ef3-0c12e3bca9fc",
   "metadata": {},
   "source": [
    "# Embeddings from pretrained model(Resnet50 default):\n",
    "\n",
    "The authors start from a scene image $I_s$ and a product image $I_p$. They use a pretrained (on ImageNet) Resnet50 and push the image through the the resnet and get back the output of the final pooling layer, which can be interpreted as an embedding (they call it visual feature, this is going to be our global similarity) vector. This gives us $\\textbf{v}_s \\in R^{d_1}$ and if we do this for the product image we similarly get $\\textbf{v}_p \\in R^{d_1}$.\n",
    "\n",
    "For the local embeddings they follow a similar procedure but take the embeddings from one of the intermediate convolutional neural networks. In the CNN the region/point in space is preserved and so image, we take the lower left corner and all the features from that corner. That will give us the local feature maps $m_i \\in R_{d_2}$ where the i denotes the point in the picture.\n",
    "\n",
    "All these embeddings are passed through a 2 layer MLP $g(\\Theta,.)$ to convert them to the same metric space, so a distance can be applied. The MLP params seem to be the same for the product and scene embeddings but different for all the location specific ones. With it we get the final \n",
    "embeddings $\\textbf{f}_s = g(\\Theta_g;v_s),\\textbf{f}_p = g(\\Theta_g;v_p), \\textbf{f}_i = g(\\Theta_l;m_i), \\hat{\\textbf{f}}_i' = g(\\Theta_l';m_i')$\n",
    "\n",
    "_The authors mention applying l2 norm penalty, this is the architecture for g() :Linear-BN-Relu-Dropout-Linear -L2Norm_\n",
    "\n",
    "In section 4.1 they mention unit normalization of the embeddings, then what they mean for l2 norm is the UnitNormalization layer from keras which normalizes using the l2 norm.\n",
    "\n",
    "If the notation in the paper is not wrong, there is parameter sharing in the global embeddings from the scene and product image, that is $\\Theta_s = \\Theta_p$ according to stack overflow this is the appropriate way to handle this: https://stackoverflow.com/questions/60094332/how-to-use-the-same-layer-model-twice-in-one-model-in-keras\n",
    "\n",
    "\n",
    "Guillem 12/04/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72c1918-2192-47ea-afca-8bf21cedde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e55d3-6267-4e32-8d98-2837b88b2a5a",
   "metadata": {},
   "source": [
    "I was done writing my function and then chatgpt was like no that's too simple not reusable enough blablabla. I checked keras docs and they mention this as the proper way to build a custom layer so I rewrote it, looks pretty much the same but some keras fancy stuff of auto shapes goes on in the background... \n",
    "\n",
    "https://www.tensorflow.org/tutorials/customization/custom_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3987bf50-baa9-4400-8c7c-f46f882b96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Implements the g(Â·) that takes the visual feature vectors (v_s, v_p global embeddings) and \n",
    "    the feature maps m_i and puts them through a small nn, where the final dimension of the embeddings \n",
    "    gets specified.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "        input_shape: works for v_s, v_p, m_i. Should be a 1*1*feature vector, it gets flattened. \n",
    "        u_intermediate: units from first dense layer, unspecified in the paper if I am not mistaken. \n",
    "        dropout: Dropout layer percentage,  unspecified in the paper if I am not mistaken. \n",
    "        f_dimension: the dimension of the final embeddings used in the attention mechanism.\n",
    "    \n",
    "    returns: \n",
    "    \n",
    "        model mapping from the input_embedding to the output f embedding. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, u_intermediate, dropout, f_dimension,name=None, **kwargs):\n",
    "        # gets all methods from the parent(inheritance) of GLayer which is keras.Layer\n",
    "        super(GLayer, self).__init__(name=name,**kwargs)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(units=u_intermediate) # no activation means linear.\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "        self.dropout = layers.Dropout(rate=dropout)\n",
    "        self.dense2 = layers.Dense(units=f_dimension)\n",
    "        self.unit_norm = layers.UnitNormalization()\n",
    "    def call(self,inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        f = self.unit_norm(x)\n",
    "        return f\n",
    "    def get_config(self):\n",
    "        # If we want to save the model we need this apparently.\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"u_intermediate\": self.dense1.units,\n",
    "            \"dropout\": self.dropout.rate,\n",
    "            \"f_dimension\": self.dense2.units, \n",
    "            \"name\": self.name\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f2850-30a2-4323-81b8-cb87398d8622",
   "metadata": {},
   "source": [
    "We should have shared weights for both global embeddings and different params for each of the local ones.\n",
    "Create one GLayer for both and #(width*length) layers for the local mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dda4320-0be3-4169-b27f-1f9827284fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn = GLayer(u_intermediate=256, dropout=0.3, f_dimension=128)\n",
    "\n",
    "# Input shape could be (batch_size, 1, 1, feature_dim)\n",
    "#out1 = gnn(input1)\n",
    "#out2 = gnn(input2)  # Shared weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc709f8d-df89-4923-aeee-bdbf379cf9bb",
   "metadata": {},
   "source": [
    "## $e_c$ embeddings\n",
    "\n",
    "A bit tricky to implement, althought it is just a bias layer that changes by category.\n",
    "\n",
    "**Forgot the unit normalization! -> Now added**\n",
    "\n",
    "_To Do_: Add docstrings and check differences between build and call... Also that final call thing that was giving me errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80dbe1c-ce03-4501-afc4-cb09b86eff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryDependentBiasLayer(layers.Layer):\n",
    "    \"\"\" \n",
    "    args: \n",
    "        num_categories: given from data \n",
    "        bias_dim: should be same dimension as f \n",
    "    returns: \n",
    "        bias vector for the correct category, should be of the same dimension as f \n",
    "    \"\"\"\n",
    "    def __init__(self, num_categories, bias_dim, **kwargs):\n",
    "        super(CategoryDependentBiasLayer, self).__init__(**kwargs)\n",
    "        self.num_categories = num_categories\n",
    "        self.bias_dim = bias_dim\n",
    "        self.unit_norm = layers.UnitNormalization()\n",
    "    def build(self, input_shape):\n",
    "        # One bias vector per category\n",
    "        self.biases = self.add_weight(\n",
    "            shape=(self.num_categories, self.bias_dim),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "            name=\"category_biases\"\n",
    "        )\n",
    "\n",
    "    def call(self, category_indices):\n",
    "        # Some stuff from chatgpt to differentiate between one input and a batch of inputs... \n",
    "        # TO DO: Check it makes sense.\n",
    "        category_indices = tf.convert_to_tensor(category_indices)\n",
    "    \n",
    "        # Handle scalar (inference) input\n",
    "        if category_indices.shape.rank == 0:\n",
    "            category_indices = tf.expand_dims(category_indices, axis=0)\n",
    "    \n",
    "        # Handle (batch_size, 1) input (common in Keras with integer inputs)\n",
    "        if category_indices.shape.rank == 2 and category_indices.shape[-1] == 1:\n",
    "            category_indices = tf.squeeze(category_indices, axis=-1)\n",
    "    \n",
    "        # Now shape should be (batch_size,)\n",
    "        biases = tf.gather(self.biases, category_indices)\n",
    "        return self.unit_norm(biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
